# Bitácora de semana 9

El trabajo de esta semana comenzó entendiendo en profundidad el funcionamiento de los módulos implementados en dspy, con los fines de 1) Entender la razón del porque los agentes no parecen "debatir" entre sí (realizan comentarios al aire), y no siguen sus personalidades descritas y 2) Preparar la arquitectura para el eventual manejo de herramientas.

Dentro de esto se descubrió que el uso de ChainOfThought y Predict eran muy elementales, y las Signatures implementadas no le permitían al modelo comprender realmente la tarea a realizar. Se implementaron Signatures por tarea, como para la generación de una respuesta, que toma como input desde el contexto, la última opinión y el background del agente; y declara como salidas el comentario, pero también al agente que está interpelando, su posición (apoyo, rebate), la cita exacta que busca rebatir, su confianza, entre otros. Esto obliga al modelo a tener mucha más consideración por la forma final de su respuesta. Además se reemplaza la generación por el módulo ReAct preparado para usar herramientas, y que permite varias iteraciones mediante el LLM. Por sobre este módulo se usa Refine, que permite mejorar la calidad de la respuesta en base a una función de recompensa. Se definió la recompensa como una combinación entre "Novelty", qué tan distinta es la nueva opinión versus la última opinión, y "PersonaFit", qué tan alineada y personficada está la opinión con el background del agente. 

Se realizaron también otras modificaciones. Se sustituyó la forma en que se obtiene el engagement de un agente, desde similarity con embeddings a una decisión tomada por el mismo agente (bajo un contexto simplificado, con un grado de "confianza" en su intervención). Esta decisión, más que apuntar a una falencia técnica, se alinea más con la búsqueda de volver a los agentes más autónomos y con menos reglas definidas de forma rígida. Se definieron Signatures para este nuevo comportamiento, y también para el voto. 

Algo clave al definir signatures fue el entregar una pequeña explicación suplementaria al nombre de cada parámetro. dspy es en naturaleza declarativo, y los autores del framework sugieren entrenar a un modelo en base a ejemplos, en lugar de hacer "prompt-engineering". El problema es que como nuestros background son generados por el usuario, no poseemos a priori ejemplos de comportamiento del agente, por lo que la explicación de los parámetros dentro del signature fue la más adecuada a este caso.

Después, se comenzó a investigar la alternativas para incluir herramientas de búsqueda en la web para los agentes (y eventualmente generar backgrounds de forma automática). Se evaluaron principalmente dos métodos de exploración: PSE (Programmable search engines) y el Modo AI de Google (disponible mediante una API como SerpApi). Dadas ventajas y limitaciones de cada uno, PSE siendo altamente confiable y determinista, pero entregando solo links, y Google AI Mode entregando insights directamente además de las fuentes en links, pero siendo menos confiable (menos aún desde una API de terceros), se decidió que se daría la opción de usar ambos. Desde esto, surgió una arquitectura de tools que describo en los diagramas de esta semana en OneDrive.

Una herramienta WebSearch tiene el modo principal mediante PSE. El usuario puede seleccionar las siguientes tools para un agente: Wikipedia Tool, News Tool, y Page Tool. Las dos últimas reciben URLs. En base a las URLs establecidas mediante estas 3 tools, se arma una whitelist para PSE. PSE entrega páginas asociadas a estos dominios. Dependiendo del tipo de página se utiliza a) La API de Wikipedia (pública), b) La librería newsletter3k y trafilatura como fallback, para la extracción de contenido de forma estructurada desde estas páginas, y c) La librería requests como recurso para extraer el contenido disponible en una página web genérica (este método es el menos confiable, pues para páginas como LinkedIn, pueden estar bloqueadas detrás de Auth). Una herramienta adicional GoogleAI Tool agrega la búsqueda mediante GoogleAI, que obtiene una respuesta directa aumentada por URLs. Estas URLs se manejan de la misma forma que las anteriores (y caen en alguno de los 3 casos). Una vez obtenidos todos los contenidos, se sumarizan juntos y se devuelven. 

Por otra parte, una herramienta Recall es la implementación de RAG para nuestros agentes. Es activada por las tools de cara al usuario a) Document Tool, que carga un documento, genera su embedding y lo guarda en la base de datos para semantic retrieval), o b) Note Tool, que habilita el registro del debate y fuentes en la base de datos también para semantic retrieval. El agente solo ve "Recall", una herramienta que recibe un query y tiene un parámetro "source" establecido dependiendo de si es que Note Tool o Document Tool hayan sido activadas. "source" podría ser "document", "notes" o "all". 

También se pueden tener otras herramientas de razonamiento, en la forma de meta-herramientas: Herramientas del agente que utilizan otras herramientas. Estas pueden ser FactChecking (toma las fuentes disponibles, contexto, webSearch, Recall, dependiendo) y hace una evaluación de veracidad en base a ellas. O Abstraction, que toma múltiples fuentes, y genera una opinión balanceada y nueva en base a ellas (no solo un contraste, sino que una síntesis).

La razón por la cual se hace la división entre "herramientas del agente" y "herramientas asignables por el usuario", es por la reutilización de componentes internos y modificaciones de comportamiento, en base a elementos que a primera vista parecen independientes. Por ejemplo, esto abstrae al usuario de entender que la lectura de documentos, y la memoria en base al debate, se basa en el mismo funcionamiento interno de semantic retrieval mediante una infra de RAG. Para la búsqueda es similar. Cada tool dice "esto le permite al agente buscar en estas páginas", y google AI dice "esto le permite buscar en Google AI". Lo que el usuario no sabe es que si se selecciona Google AI, también se utilizará para aumentar las búsquedas en base a nuevas fuentes, usando los mismos mecanismos específicos que se utilizan para las otras herramientas. 

Lo último que se hizo esta semana fue agregar la opción para límites de habla por agente. Un agente tendría una cantidad limitada de intervenciones. Si llega a su límite, no volverá a ser considerado para un nuevo turno de habla. Esto está inspirado en los límites de tiempo en los debates reales. También fue agregado a la consideración de la generación de una respuesta: El agente sabrá al momento de generar una respuesta cuando turnos le quedan. De esta forma puede saber si debe cerrar sus ideas y priorizar su posición final o no.